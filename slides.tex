\documentclass{beamer}
\input{frontmatter/preamble_slides}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Solar]{Solar: $L_0$ path averaging for fast and accurate variable selection in high-dimensional and large-scale data}

\author{Ning Xu}
\date{\today}


\begin{document}

\begin{frame}
  %
  \titlepage
  %
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------

\section{Motivation}

\begin{frame}{Resources and Acknoledgements}
  %
  \begin{itemize}
    %
    \item My sincere thanks to Pierre Del Moral (INRIA) for careful comments as well as seminar participants at Google, NICTA, Monash University, etc.
    %
    \item Paper: \url{https://arxiv.org/abs/2007.15707}
    %
    \item Code: \url{https://github.com/isaac2math/solarpy}
    %
    \end{itemize}
  %
\end{frame}

\begin{frame}{Motivation}

  \begin{itemize}
    %
    \item The balance of computation load and performance:
    \begin{itemize}
      %
      \item compute more if you want to improve the performance.
      %
    \end{itemize}
    
    \item Optimization : a clear performance boost with a substantial reduction of the computation load.
    
    \begin{itemize}
      %
      \item We reconstruct the algorithm from its foundation.
      %
    \end{itemize}  
  \end{itemize}

\end{frame}

\begin{frame}{Motivation}

  \begin{itemize}    
    %
    \item Recent innovations to lasso-type algorithms have largely addressed some issues like
    \begin{itemize}
      %
      \item selection of redundant variables;
      %
      \item rejection of informative variables; 
      %
      \item poor performance under high multicollinearity in high dimensional ($p>n$) and large scale data (large $p$ and large $n$). 
      %
    \end{itemize} 
    %
    \item However, in alleviating old problems, the innovations have revealed new challenges.
    %
  \end{itemize}

\end{frame}


\subsection{Recent innovations to lasso-type algorithms and their issues}


\begin{frame}{Bootstrap variable selection}
  %
  \begin{itemize}
      %
      \item Bootstrap selection \citep{bach2008bolasso,meinshausen2010stability,wang2011random,mameli2017estimating} markedly improves selection sparsity and inference accuracy,
      %
      \item It requires repeating the selection algorithms (and relevant hyperparameter tuning methods) on hundreds of bootstrap subsamples to average selection or inference results, limiting applicability in large scale data such as DNA sequencing, image recognition, fMRI and MRI neuroimaging data, and natural language processing (where both $p$ and $n$ are often over $10,000$). 
      %
      \item choosing the bootstrap variable selection threshold is even more computationally expensive and tricky \citep{bach2008bolasso, huang2014stat}. The pre-defined threshold may omit informative variables (low power) and select redundant variables (high false discovery rate) in both high and low dimensions.;
      %
  \end{itemize}
  %
\end{frame}


\begin{frame}{Post-selection rule}
  %
  \begin{itemize}
    %
    \item Post-lasso selection rules \citep{ghaoui2010safe, tibshirani2012strong} improve lasso selection sparsity without increasing the computation burden.
    
    \item However, recent research \citep{wang2014safe, zeng2017efficient} and our paper suggest that, under complicated dependence structures, both rules may be prone to rejecting informative variables, selecting redundant variables, or proposing repeated modifications (e.g., rejecting a variable in an early round and adding it back in a later round).
    %
  \end{itemize}
  %
\end{frame}


\begin{frame}{Data-splitting hypothesis tests}

  \begin{itemize}
    %
    \item Data-splitting hypothesis tests \citep{wasserman2009high, meinshausen2009p,romano2019multiple, diciccio2020exact} splits the dataset in two: one part for variable selection, the other for testing. 
    
    \item To improve test power, data splitting is repeated on each bootstrap subsample, raising similar computational concerns as bootstrap selection \citep{bach2008bolasso}.
    
    \item Data splitting reserves some of the data for variable selection. It reduces the degrees of freedom for testing on the remaining data, presenting a challenge to detect weak signals when sample size is limited. \citep{diciccio2020exact}
    %
  \end{itemize}
\end{frame}


\begin{frame}{Variable screening}

  \begin{itemize}
    %
    \item The variable screening algorithm \citep{fan2008sure, hall2009using,hall2009usingb, li2012robust, li2012feature} efficiently ranks the absolute values of the unconditional correlations between each covariate and the response variable, selecting only the top-ranked variables.
    
    \item \citet{fan2008sure}, \citet{barut2016conditional}, and our paper show that variable screening also suffers from selection of redundant variables and rejection of informative variables when the dependence structures in the data are complicated. 
    %
  \end{itemize}
\end{frame}

\subsection{Our improvements}

\begin{frame}{Solar algorithm}
  %
  To address these issues, we propose a new forward selection algorithm: \emph{subsample-ordered least-angle regression (solar)} and its coordinate-descent generalization \emph{solar-cd}.

  \begin{itemize}  
    %
    \item Solar constructs a solution path for a subsample using the $L_0$ norm and then averages solution paths across subsamples.
     
    \item Path averaging retains the ranking information of the informative variables while averaging out sensitivity to high dimensionality, improving variable selection stability, efficiency, and accuracy. 
    
    \item Because it uses the same numerical optimizers as lasso, solar may be generalized to many lasso variants.
    %
  \end{itemize}
  %
\end{frame}


\begin{frame}{Theoretical properties}
  %
  Under the \citet{zhang09} general framework of forward selection, we prove that:

  \begin{itemize}  
    \item  [(i)] with a high probability, path averaging perfectly separates informative variables from redundant variables on the average $L_0$ path; 
    
    \item  [(ii)] solar variable selection is consistent and accurate under the general framework of forward selection; 
    
    \item  [(iii)] the probability that solar omits weak signals is restricted by sample size, the sparse eigenvalue condition, and the stopping condition.
    %
  \end{itemize}
  %
\end{frame}


\begin{frame}{Computational properties}
  %
  Using simulations, examples, and real-world data, we demonstrate the following advantages of solar:
  %
  \begin{itemize}  
    %
    \item  [(i)] with less than $1/3$ of the lasso computation load, solar outperforms lasso in terms of the sparsity (64-84\% reduction in redundant variable selection) and accuracy of variable selection;
    
    \item  [(ii)] compared with the lasso safe/strong rule and variable screening, solar largely avoids selection of redundant variables and rejection of informative variables in the presence of complicated dependence structures and harsh settings of the irrepresentable condition;
    
    \item  [(iii)] the sparsity and stability of solar conserves true signals and residual degrees of freedom for data-splitting hypothesis testing, improving the accuracy of post-selection inference on weak signals with limited $n$;  
    %
  \end{itemize}
  %
\end{frame}

\begin{frame}{Computational advantages}
  %
  \begin{itemize}  
    %
    \item  [(iv)] replacing lasso with solar in bootstrap selection (e.g., bolasso or stability selection) produces a multi-layer variable ranking scheme that improves selection sparsity and ranking accuracy with the computation load of only one lasso realization; 
    
    \item  [(v)]  solar bootstrap selection is substantially faster (98\% lower computation time) than the theoretical maximum speedup for parallelized bootstrap lasso (confirmed by Amdahl's law). The efficiency of bootstrap solar makes cross validation (CV) computationally affordable for optimizing the bootstrap selection threshold even in large scale and high dimensional data
    %
  \end{itemize}
  %
\end{frame}

\begin{frame}[allowframebreaks]
  \bibliographystyle{elsarticle-harv}
  \bibliography{ref/CVrefs}
\end{frame}



\end{document}
